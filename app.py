import streamlit as st

# Sample dataset: 0 = human-written, 1 = AI-generated
data = [
    ("This text was generated by an AI model.", 1),
    ("The AI-generated content is becoming more advanced.", 1),
    ("Machine learning models can generate realistic text.", 1),
    ("AI-generated text is often indistinguishable from human writing.", 1),

    ("This is a human-written sentence.", 0),
    ("I enjoy reading books and writing stories.", 0),
    ("Natural language processing is a fascinating field.", 0),
    ("I love spending time with my family and friends.", 0),
]

# --- Text preprocessing ---
def preprocess(text):
    text = text.lower()
    return "".join(char for char in text if char.isalpha() or char.isspace())

def tokenize(text):
    return text.split()

# --- Vocabulary creation ---
vocab = set()
for text, _ in data:
    tokens = tokenize(preprocess(text))
    vocab.update(tokens)
vocab = list(vocab)

def text_to_vector(text):
    vector = [0] * len(vocab)
    tokens = tokenize(preprocess(text))
    for token in tokens:
        if token in vocab:
            vector[vocab.index(token)]
